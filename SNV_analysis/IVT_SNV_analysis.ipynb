{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5b0844-fca5-404a-ae27-05626fe063ff",
   "metadata": {},
   "source": [
    "# IVT SNV Identification and Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a0eff-8b4f-46c3-9263-e718c77559f4",
   "metadata": {},
   "source": [
    "### Imports\n",
    "The following libraries are used for this analysis. Additionally this program needs to be run from a linux or unix environment and have Samtools 1.16 installed and accessible through python sys commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7787f897-ec1f-4f4d-ac21-b4d3920b1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f75768-a1e4-40aa-b1e6-fe00a83b0060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1679403235.635107"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Make directories with sys.command()\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e04ec-1e6c-43c3-8c19-8347a18b2d39",
   "metadata": {},
   "source": [
    "### Input Files:\n",
    "1. Path to the reference genome, this must be the same reference genome used to produce the pysamstats pileups and alignments.\n",
    "2. Path to the gencode .gff3 file from which to extract positions of interest.\n",
    "3. A list of paths to the pysamstats pileup to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b407ded-5709-4acd-a82b-30511880bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit these to match path for system running this program\n",
    "ref_path = \"./GRCh38.p10.genome.fa\"\n",
    "gff3_path =  'gencode.v38.annotation.gff3'\n",
    "IVT_Files =  ['IVT1_pysamstats.txt','IVT2_pysamstats.txt','IVT3_pysamstats.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36aeb73-59be-4362-99a2-c124dfe5d822",
   "metadata": {},
   "source": [
    "### Extract positions of Interest from GFF3 File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409ccf6-5276-4c7d-8b01-7f17517d4130",
   "metadata": {},
   "source": [
    "We identify all regions that are labeled as \"Protein coding\" and \"Gene\" by gencode. From these regions we take the start and stop position and pay the 1 time cost of building sets of those positions by chromosome. These sets are written to disk and loaded as needed during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861384e-a05a-44ce-aa92-2104aba3d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a dictionary for gene look up table. \n",
    "# Keys will the (chr, start, stop) for each gene we're examining.\n",
    "gene_dict = {}\n",
    "for line in open(gff3_path, 'r'):\n",
    "    if 'gene_type=protein_coding' in line and line.split()[2] == 'gene' and line.split()[0] != 'chrY':\n",
    "        split_line = line.split()\n",
    "        chrom = split_line[0]\n",
    "        start = int(split_line[3])\n",
    "        stop = int(split_line[4])\n",
    "        gene_dict[(chrom, start, stop)] = {}\n",
    "        tag_info = split_line[-1].split(';')\n",
    "        for info in tag_info:\n",
    "            info_split = info.split('=')\n",
    "            gene_dict[(chrom, start, stop)][info_split[0]] = info_split[1]\n",
    "\n",
    "if 'acceptable_positions_sets' not in os.listdir(os.getcwd()):\n",
    "#TODO: mkdir acceptable position sets\n",
    "    #Build a set of all chromosomes found in the gencode dictionary\n",
    "    chroms = set()\n",
    "    for gene in gene_dict:\n",
    "        chroms.add(gene[0])\n",
    "    chroms = list(chroms)\n",
    "    \n",
    "    #For each chromosome construct a set of acceptable positions (ie. within the bounding\n",
    "    #start and stop positions of a 'protein coding' 'gene' from gencode)\n",
    "    for chrom in chroms:\n",
    "        acceptable_positions_set = set()\n",
    "        for positions in gene_dict:\n",
    "            if positions[0] != chrom:\n",
    "                continue\n",
    "            for i in range(positions[1], positions[2] + 1):\n",
    "                acceptable_positions_set.add((positions[0], i))\n",
    "        positions_set_fh = open(f'./acceptable_position_sets/{chrom}_acceptable_positions_set', 'wb')\n",
    "        pickle.dump(acceptable_positions_set, positions_set_fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4beb13-27dc-4e6b-a055-fc6e2abee698",
   "metadata": {},
   "source": [
    "### Identify all SNVs reported by Pysamstats "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a40386-b57b-41d4-a081-f5ae6fdf6cce",
   "metadata": {},
   "source": [
    "Each line of the pysamstats pile up will be evaluated for two things to determine if it should be included in the first pass of our SNV set: \n",
    "1. Is the position in our acceptable positions set? ie. is it part of a protein coding gene region?\n",
    "2. Is there at least 1 reported base that differs from the reference genome?\n",
    "If both of these conditions are passed, we will include this in our first pass of SNV dict. This step will be repeated for each Pysamstats pileup provided in the IVT_Files variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1fbbc-80f6-4469-9357-62f1c477687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list of chromosomes can be reduced if analysis is only desired for a subset\n",
    "chroms = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12',\n",
    "          'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrM']\n",
    "#Iterate over each of pileups. These are kept seperate due to memory constraints. They will be written seperately, then combined\n",
    "#at a later step of the program.\n",
    "for IVT in IVT_Files:\n",
    "    print(f\"{IVT}\")\n",
    "    \n",
    "    #Gene_Position_dict will be our primary data structure and will use the position as\n",
    "    #a key to store relevant information from pysamstats.\n",
    "    Gene_Position_dict = {}\n",
    "    IVT_name = IVT.split('_')[0]\n",
    "    \n",
    "    #Starting with chr1 load acceptables position sets. Since our pysamstats pileups are sorted\n",
    "    #we know that once we get to a new chromosome we won't revisit an old one. This means that we\n",
    "    #can write the old chromosome Gene_position_dict to disk and move on.\n",
    "    acceptable_positions_set = pickle.load(open('acceptable_position_sets/chr1_acceptable_positions_set', 'rb'))\n",
    "    acceptable_chrom = 'chr1'\n",
    "    for line in open(IVT, 'r'):\n",
    "        \n",
    "        #Don't count the starting line of pysamstats file\n",
    "        if 'reads_all' in line: #Start line?\n",
    "            continue\n",
    "            \n",
    "        split_line = line.split()\n",
    "        \n",
    "        #Exclude information from chromosomes not in our chroms set\n",
    "        if split_line[0] not in chroms:\n",
    "            continue\n",
    "            \n",
    "        read_pos = int(split_line[1])\n",
    "        \n",
    "        #If our current line is from a different chromosome than our previous line\n",
    "        #we need to write the dictionary from the old chromosome to disk, and set up\n",
    "        #a new dictionary and continue on.\n",
    "        if split_line[0] != acceptable_chrom:\n",
    "            dump_dict_fh = open(f'./REBASE_Pysamstats_Dicts/{IVT}_{acceptable_chrom}_pysamstats_dict', 'wb')\n",
    "            pickle.dump(Gene_Position_dict, dump_dict_fh)\n",
    "            print(f\"Dumped {acceptable_chrom} dict\")\n",
    "            Gene_Position_dict = {}\n",
    "            acceptable_positions_set = pickle.load(open(f'acceptable_position_sets/{split_line[0]}_acceptable_positions_set', 'rb'))\n",
    "            acceptable_chrom = split_line[0]\n",
    "            print(f\"Processing reads that are on {acceptable_chrom}\")\n",
    "        \n",
    "        #If the position is not in our calculated set exclude it\n",
    "        if (split_line[0], read_pos) not in acceptable_positions_set:\n",
    "            continue\n",
    "        \n",
    "        #Document all relevant information; See ReadMe for restrictions on pysamstats file format\n",
    "        Gene_Position_dict[read_pos] = {}\n",
    "        Gene_Position_dict[read_pos][\"Ref\"] = split_line[2]\n",
    "        Gene_Position_dict[read_pos][\"Reads\"] = int(split_line[3])\n",
    "        Gene_Position_dict[read_pos][\"A\"] = int(split_line[6])\n",
    "        Gene_Position_dict[read_pos][\"C\"] = int(split_line[7])\n",
    "        Gene_Position_dict[read_pos][\"T\"] = int(split_line[8])\n",
    "        Gene_Position_dict[read_pos][\"G\"] = int(split_line[9])\n",
    "        Gene_Position_dict[read_pos][\"Deletions\"] = int(split_line[4])\n",
    "        Gene_Position_dict[read_pos][\"Insertions\"] = int(split_line[5])\n",
    "\n",
    "    #Write our final chromosome to disk\n",
    "    dump_dict_fh = open(f'./REBASE_Pysamstats_Dicts/{IVT_name}_{acceptable_chrom}_pysamstats_dict', 'wb')\n",
    "    pickle.dump(Gene_Position_dict, dump_dict_fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36be7f2-15f0-4886-ac16-c008029026de",
   "metadata": {},
   "source": [
    "### Merge dictionaries from each of the pysamstats pileups into a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea95e05-2b22-4d76-9fa4-dcc318785aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine the various dictionaries\n",
    "for chrom in chroms:\n",
    "    combined_chrom_dict = {}\n",
    "    for IVT in IVT_Files:\n",
    "        print(f\"{chrom} : {IVT}\")\n",
    "        IVT_name = IVT.split('_')[0]\n",
    "        file_path = f\"./REBASE_Pysamstats_Dicts/{IVT_name}_pysamstats.txt_{chrom}_pysamstats_dict\"\n",
    "        fh = open(file_path, 'rb')\n",
    "        d = pickle.load(fh)\n",
    "        fh.close()\n",
    "        \n",
    "        #Either add a position to the dictionary or add current values to existing values\n",
    "        for key in d:\n",
    "            if key not in combined_chrom_dict:\n",
    "                combined_chrom_dict[key] = d[key]\n",
    "            else:\n",
    "                combined_chrom_dict[key][\"A\"] += d[key][\"A\"]\n",
    "                combined_chrom_dict[key][\"T\"] += d[key][\"T\"]\n",
    "                combined_chrom_dict[key][\"G\"] += d[key][\"G\"]\n",
    "                combined_chrom_dict[key][\"C\"] += d[key][\"C\"]\n",
    "                combined_chrom_dict[key][\"Deletions\"] += d[key][\"Deletions\"]\n",
    "                combined_chrom_dict[key][\"Insertions\"] += d[key][\"Insertions\"]\n",
    "                combined_chrom_dict[key][\"Reads\"] += d[key][\"Reads\"]\n",
    "    out_file_path = f\"./combined_pysamstats/{chrom}_pysamstats_dict\"\n",
    "    fh = open(out_file_path, 'wb')\n",
    "    pickle.dump(combined_chrom_dict, fh)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b737e-edaf-4ccf-82a7-b95a63fcf5c4",
   "metadata": {},
   "source": [
    "### Minimum 30% SNV Occurence Threshold & 10 minimum Reads Threshold\n",
    "Here we will pare down our SNV dataset to only include locations where at least 30% of the calculated reads have an SNV. This will help to minimize the amount of nanopore noise entering our analysis. Additionally we will only include locations with a minimum of 10 reads, this is done to minimize extraneous alignments being reported as true. Additionally a minimum of 10 reads lends some weight to our minimum SNV occurence threshold. One off events will be prevented from entering the dataset.\n",
    "These variables can be changed if an individual wants to perform this analysis again with alternate intentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a58fab-ff02-42a8-9a4a-34c5bbc3e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold = 0.3\n",
    "min_reads = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27344d26-822b-4ea2-b872-a7cb67bfc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\"A\", \"C\", \"T\", \"G\"]\n",
    "SNP_dict_30 = dict()\n",
    "for file in os.listdir(os.path.join(os.getcwd(), \"combined_pysamstats\")):\n",
    "    print(file)\n",
    "    chrom = file.split(\"_\")[0]\n",
    "    fh = open(f\"./combined_pysamstats/{file}\", 'rb')\n",
    "    tmp_dict = pickle.load(fh)\n",
    "    fh.close()\n",
    "    for key in tmp_dict:\n",
    "        ref = tmp_dict[key]['Ref']\n",
    "        #We calculated the number of reads as Every A, T, C, G, and Deletion. Insertions were ommited since they\n",
    "        #do not represent this position in the pileup, but rather represent a Genomic event\n",
    "        reads = int(tmp_dict[key]['A']) + int(tmp_dict[key]['T']) + int(tmp_dict[key]['G']) + int(tmp_dict[key]['C']) + int(tmp_dict[key]['Deletions'])\n",
    "        if reads < min_reads:\n",
    "            continue\n",
    "        for option in options:\n",
    "            if option == ref:\n",
    "                continue\n",
    "            if int(tmp_dict[key][option]) > int(min_threshold*reads):\n",
    "                SNP_dict_30[(chrom, key, option)] = tmp_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a39c2-8a6c-43c2-9117-0cf1475cefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snp_dict_with_threshold(SNP_dict_30, threshold):\n",
    "    SNP_dict_out = dict()\n",
    "    for key in SNP_dict_30:\n",
    "    ref = SNP_dict_30[key]['Ref']\n",
    "    reads = int(SNP_dict_30[key]['A']) + int(SNP_dict_30[key]['T']) + int(SNP_dict_30[key]['G']) + int(SNP_dict_30[key]['C']) + int(SNP_dict_30[key]['Deletions'])\n",
    "    if int(SNP_dict_30[key][key[2]]) > int(threshold*reads):\n",
    "        SNP_dict_out[key] = SNP_dict_30[key]\n",
    "    return SNP_dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f95ac-c6fa-4360-bd75-e1bc8cf2fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP_dict_40 = snp_dict_with_threshold(SNP_dict_30, 0.4)\n",
    "SNP_dict_50 = snp_dict_with_threshold(SNP_dict_30, 0.5)\n",
    "SNP_dict_60 = snp_dict_with_threshold(SNP_dict_30, 0.6)\n",
    "SNP_dict_70 = snp_dict_with_threshold(SNP_dict_30, 0.7)\n",
    "SNP_dict_80 = snp_dict_with_threshold(SNP_dict_30, 0.8)\n",
    "SNP_dict_95 = snp_dict_with_threshold(SNP_dict_30, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f586c-1a67-4866-959d-c39fad35c19b",
   "metadata": {},
   "source": [
    "### Bin SNVs: Low confidence Kmers, Ensembl Variants, Putative IVT SNVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d371fbbe-ca1f-465d-97bb-c626a2556896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_bin_and_write_to_file(snp_dict, file_id):\n",
    "    #Filter based on known variants\n",
    "    fh = open('gvf_set', 'rb')\n",
    "    gvf_set = pickle.load(fh)\n",
    "    fh.close()\n",
    "    gvf_filter_results = snps_in_gvf(gvf_set, snp_dict)\n",
    "    gvf_dict = gvf_filter_results[2]\n",
    "    gvf_filtered_snp_dict = gvf_filter_results[1]\n",
    "    gvf_count = gvf_filter_results[0]\n",
    "    \n",
    "    #Filter based on problematic Kmers\n",
    "    fh = open('kmer_set', 'rb')\n",
    "    kmer_set = pickle.load(fh)\n",
    "    fh.close()\n",
    "    prob_kmers_results = count_prob_kmers(gvf_filtered_snp_dict, kmer_set)\n",
    "    prob_kmer_dict = prob_kmers_results[2]\n",
    "    final_snp_dict = prob_kmers_results[1]\n",
    "    prob_kmer_count = prob_kmers_results[0]\n",
    "    \n",
    "    #Write Known variations to file\n",
    "    fh = open(f\"./Output_data/{file_id}_known_variants.tsv\", 'w')\n",
    "    fh.write(\"Chrom\\tPosition\\tVariant\\tReference\\tA_count\\tC_count\\tT_count\\tG_count\\tDel_count\\tInsert_count\\n\")\n",
    "    for key in gvf_dict:\n",
    "        line = f\"{key[0]}\\t{key[1]}\\t{key[2]}\\t{snp_dict_value_to_tab_string(gvf_dict[key])}\\n\"\n",
    "        fh.write(line)\n",
    "    fh.close()\n",
    "    \n",
    "    #Write Problem kmer variations to file\n",
    "    fh = open(f\"./Output_data/{file_id}_problem_kmer_variants.tsv\", 'w')\n",
    "    fh.write(\"Chrom\\tPosition\\tVariant\\tReference\\tA_count\\tC_count\\tT_count\\tG_count\\tDel_count\\tInsert_count\\n\")\n",
    "    for key in prob_kmer_dict:\n",
    "        line = f\"{key[0]}\\t{key[1]}\\t{key[2]}\\t{snp_dict_value_to_tab_string(prob_kmer_dict[key])}\\n\"\n",
    "        fh.write(line)\n",
    "    fh.close()\n",
    "    \n",
    "    #Write novel variations to file\n",
    "    fh = open(f\"./Output_data/{file_id}_novel_IVT_variants.tsv\", 'w')\n",
    "    fh.write(\"Chrom\\tPosition\\tVariant\\tReference\\tA_count\\tC_count\\tT_count\\tG_count\\tDel_count\\tInsert_count\\n\")\n",
    "    for key in final_snp_dict:\n",
    "        line = f\"{key[0]}\\t{key[1]}\\t{key[2]}\\t{snp_dict_value_to_tab_string(final_snp_dict[key])}\\n\"\n",
    "        fh.write(line)\n",
    "    fh.close()\n",
    "\n",
    "def count_prob_kmers(snp_dict, prob_kmer_set):\n",
    "    count = 0\n",
    "    prob_kmers = {}\n",
    "    filtered_dict = {}\n",
    "    p_to_ref = \"./ref/GRCh38.p10.genome.fa\"\n",
    "    for key in snp_dict:\n",
    "        chrom = key[0]\n",
    "        pos = int(key[1])\n",
    "        kmer = get_ref_seq(p_to_ref, chrom, pos - 4, pos + 4)\n",
    "        if kmer in prob_kmer_set:\n",
    "            count += 1\n",
    "            prob_kmers[key] = snp_dict[key]\n",
    "        else:\n",
    "            filtered_dict[key] = snp_dict[key]\n",
    "    return (count, filtered_dict, prob_kmers)\n",
    "\n",
    "def get_ref_seq(path_to_ref, ref_section, start_on_ref, end_on_ref):\n",
    "    ref_seq = subprocess.run(['samtools', 'faidx', f\"{path_to_ref}\", f\"{ref_section}:{start_on_ref}-{end_on_ref}\"],\n",
    "                             stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    ref_seq = ref_seq.split('\\n')[1:]\n",
    "    ref_seq = \"\".join(ref_seq)\n",
    "    return ref_seq\n",
    "\n",
    "def snps_in_gvf(gvf, snp_dict):\n",
    "    accounted_for = 0\n",
    "    filtered_dict = {}\n",
    "    gvf_dict = {}\n",
    "    for key in list(snp_dict.keys()):\n",
    "        if key in gvf:\n",
    "            accounted_for += 1\n",
    "            gvf_dict[key] = snp_dict[key]\n",
    "        else:\n",
    "            filtered_dict[key] = snp_dict[key]\n",
    "    return (accounted_for, filtered_dict, gvf_dict)\n",
    "\n",
    "def snp_dict_value_to_tab_string(snp_dict_value):\n",
    "    s = \"\"\n",
    "    for key in snp_dict_value:\n",
    "        if key == 'Reads':\n",
    "            continue\n",
    "        s = s + f\"{snp_dict_value[key]}\\t\"\n",
    "    s = s.rstrip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cef88f-a857-4265-8269-3ccda1804d08",
   "metadata": {},
   "source": [
    "### Define Low Quality 9mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f59a92c7-b853-4c8d-8646-1f6ec126a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fastq_9mer_quality:\n",
    "    def __init__(self):\n",
    "        self.file_dicts = {}\n",
    "    \n",
    "    def read_files(self, list_of_files):\n",
    "        for file in list_of_files:\n",
    "            file_key = file.split(\"/\")[-1][:-6]\n",
    "            self.file_dicts[file_key] = {}\n",
    "            self.parse_fastq(file)\n",
    "        \n",
    "    def parse_fastq(self, file_path):\n",
    "        i = 0\n",
    "        for line in open(file_path, 'r'):\n",
    "            i += 1\n",
    "            if i == 1:\n",
    "                seq_id = line\n",
    "            elif i == 2:\n",
    "                fasta = line.rstrip()\n",
    "            elif i == 3:\n",
    "                continue\n",
    "            elif i == 4:\n",
    "                fastq = line.rstrip()\n",
    "                self.qual_9mer(fasta, fastq, file_path.split(\"/\")[-1][:-6])\n",
    "                i = 0\n",
    "               \n",
    "    def qual_9mer(self, fasta, fastq, file_key):\n",
    "        assembled_fasta = fasta\n",
    "        assembled_q = fastq\n",
    "        for i in range(4, len(assembled_fasta) - 4):\n",
    "            niner = \"\".join(assembled_fasta[i-4:i+5])\n",
    "            qual_niner = self.niner_avg_qual(assembled_q[i-4:i+5])\n",
    "            if niner not in self.file_dicts[file_key]:\n",
    "                self.file_dicts[file_key][niner] = (qual_niner, 1)\n",
    "            else:\n",
    "                tup = self.file_dicts[file_key][niner]\n",
    "                avg_qual = (tup[0] * tup[1] + qual_niner) / (tup[1] + 1)\n",
    "                new_tup = (avg_qual, tup[1] + 1)\n",
    "                self.file_dicts[file_key][niner] = new_tup\n",
    "            \n",
    "    \n",
    "    def niner_avg_qual(self, qual_list):\n",
    "        tot_qual = sum([ord(x) - 33 for x in qual_list])\n",
    "        avg_qual = tot_qual / len(qual_list)\n",
    "        return avg_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f80d90-ef71-423b-b16b-757681f5de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_niner = fastq_9mer_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47b21a00-5be1-4f30-8b82-b7130582717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of file paths for 9mer quality analysis\n",
    "files = [\"../UCSC_Guppy_6.3.2_rna_hac.NoU.fastq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59ed09-1fc1-4631-8441-a17768927f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_niner.read_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84f569d4-4600-4397-be65-a67daf2407d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(df.index.to_list()[196608:])\n",
    "fh = open('kmer_set', 'wb')\n",
    "pickle.dump(s, fh)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c7abe-ff9c-4d7f-a685-bf59bf137de8",
   "metadata": {},
   "source": [
    "### Define Known Variants from Ensembl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560b6b6-99df-4d54-a405-7605e47a3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make gvf set\n",
    "gvf_set = set()\n",
    "for file in os.listdir(\"./ref\"):\n",
    "    print(file)\n",
    "    chrom = file.split(\"-\")[-1].split(\".\")[0]\n",
    "    if \"homo_sapien\" not in file:\n",
    "        continue\n",
    "    else:\n",
    "        for line in open(f\"./ref/{file}\"):\n",
    "            if \"SNV\" not in line and \"deletion\" not in line:\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            pos = int(split_line[3])\n",
    "            if (chrom, pos) not in SNP_30:\n",
    "                continue\n",
    "            info = split_line[-1].split(\";\")\n",
    "            var_seq = ''\n",
    "            for element in info:\n",
    "                if 'Variant_seq' in element:\n",
    "                    var_seq = element.split(\"=\")[-1]\n",
    "            gvf_set.add((chrom, pos, var_seq))\n",
    "fh = open('gvf_set', 'wb')\n",
    "pickle.dump(gvf_set, fh)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a7150-2043-417d-b1a2-3f28dbbf431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_bin_and_write_to_file(SNP_dict_95, 'v95')\n",
    "variant_bin_and_write_to_file(SNP_dict_80, 'v80')\n",
    "variant_bin_and_write_to_file(SNP_dict_70, 'v70')\n",
    "variant_bin_and_write_to_file(SNP_dict_60, 'v60')\n",
    "variant_bin_and_write_to_file(SNP_dict_50, 'v50')\n",
    "variant_bin_and_write_to_file(SNP_dict_40, 'v40')\n",
    "variant_bin_and_write_to_file(SNP_dict_30, 'v30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68b7b7-899f-4fb8-bce5-7517bafa48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time()\n",
    "print(f\"Total running time = {round((t2 - t1) / 3600, 2)} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
